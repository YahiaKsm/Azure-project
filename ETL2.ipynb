{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "\n",
        "from notebookutils import mssparkutils\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "container_landing = \"abfs://landing@storageXtarget.dfs.core.windows.net\"\n",
        "container_curated = \"abfs://curated@storageXtarget.dfs.core.windows.net\"\n",
        "\n",
        "flow_folder_path= \"Flux_flow_&_2\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get the file path of the last EXTRACTION_DATE folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "extraction_path = f\"{container_landing}/{flow_folder_path}/\"\n",
        "\n",
        "list_dates = mssparkutils.fs.ls(extraction_path)\n",
        "last_date_path = sorted(list_dates, key=lambda x: x.name, reverse=True)[0].path\n",
        "file_path = mssparkutils.fs.ls(last_date_path)[0].path\n",
        "\n",
        "print(\"landing file path:\")\n",
        "print(file_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Mounting the landing container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.fs.mount( \n",
        "    \"abfss://landing@storageXtarget.dfs.core.windows.net\", \n",
        "    \"/mnt\",\n",
        "    {\"LinkedService\": \"ls_asa_ws_X_WorkspaceDefaultStorage\"} \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "job_id = mssparkutils.env.getJobId()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Getting the last extraction date JSON file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "relative_path = \"/\".join(file_path.split(\"/\")[3:])\n",
        "path_prefix = f\"/synfs/{job_id}/mnt\"\n",
        "\n",
        "new_file_path = f\"{path_prefix}/{relative_path}\"\n",
        "\n",
        "print(\"The updated file path is :\", new_file_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Reading the JSON file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Read the file by using a mount path\n",
        "with open(new_file_path) as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Applying transformations to the JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Flatten the JSON data including the nested array:\n",
        "df_array_normalized = pd.json_normalize(data, \n",
        "                                        record_path=[\"Contents\", \"reply\", \"endpoints\"], \n",
        "                                        record_prefix=\"Contents_reply_endpoints_\", \n",
        "                                        sep=\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Flatten the JSON file excluding the nested array:\n",
        "df_keys = pd.json_normalize(data, \n",
        "                            sep=\"_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Merge the DataFrames\n",
        "df_flattened = pd.merge(df_keys, df_array_normalized, how=\"cross\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Mapping the data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Getting the snapshot current date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the current UTC timestamp\n",
        "now = datetime.utcnow()\n",
        "\n",
        "# Create the folder name with today's date\n",
        "snapshot = now.strftime(\"%Y-%m-%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Saving the file as parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "file_name = (file_path.split(\"/\")[-1]\n",
        "                      .replace(\" \", \"_\")\n",
        "                      .replace(\".json\", \"\"))\n",
        "\n",
        "# Create the full folder path\n",
        "flow_2_curated_path = f\"{container_curated}/{flow_folder_path}/flow_&_2.parquet/SNAPSHOT={snapshot}/{file_name}.snappy.parquet\"\n",
        "\n",
        "print(\"Curated file path :\")\n",
        "print(flow_2_curated_path)\n",
        "\n",
        "# Saving the file in a parquet format:\n",
        "df_flattened.to_parquet(flow_2_curated_path, index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Unmounting the container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.fs.unmount(\"/mnt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
