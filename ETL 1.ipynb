{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from io import StringIO\n",
        "\n",
        "from notebookutils import mssparkutils\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "container_landing = \"abfs://landing@storageXtarget.dfs.core.windows.net\"\n",
        "container_curated = \"abfs://curated@storageXtarget.dfs.core.windows.net\"\n",
        "\n",
        "flow_folder_path = \"Flux_flow-1\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Retrieve the last date of the extraction date folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "extraction_path = f\"{container_landing}/{flow_folder_path}/\"\n",
        "\n",
        "list_dates = mssparkutils.fs.ls(extraction_path)\n",
        "last_date_path = sorted(list_dates, key=lambda x: x.name, reverse=True)[0].path\n",
        "file_path = mssparkutils.fs.ls(last_date_path)[0].path\n",
        "\n",
        "print(\"landing file path:\")\n",
        "print(file_path)\n",
        "print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Reading CSV file from landing as pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_flow1 = pd.read_csv(file_path, header=None)\n",
        "# Convert dataframe to csv to remove double quotes\n",
        "csv_flow1 = df_flow1.to_csv(index=False, header=False)\n",
        "# Remove semicolons at the end of each row\n",
        "csv_flow1_clean = csv_flow1.replace(\";\\n\", \"\\n\")\n",
        "\n",
        "print(\"Clean CSV :\")\n",
        "print(csv_flow1_clean)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a file-like object from the CSV data\n",
        "csv_file = StringIO(csv_flow1_clean)\n",
        "flow1_cols = [\n",
        "    \"Entity\",\n",
        "    \"spf_mean\",\n",
        "    \"spf_score\",\n",
        "    \"spf_count\",\n",
        "    \"wp_mean\",\n",
        "    \"wp_score\",\n",
        "    \"wp_count\"\n",
        "]\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df_flow1_clean = pd.read_csv(csv_file, delimiter=\";\", index_col=False, usecols=flow1_cols)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding Pole column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "entity_mapping = {\n",
        "    \"X STADIUM\" : \"X CONCESSIONS\",\n",
        "    \"X AIRPORTS\" : \"X CONCESSIONS\",\n",
        "    \"X RAILWAYS\" : \"X CONCESSIONS\",\n",
        "    \"X CONCESSIONS HOLDING\" : \"X CONCESSIONS\",\n",
        "    \"X HIGHWAYS\" : \"X CONCESSIONS\"\n",
        "}\n",
        "\n",
        "df_flow1_clean[\"Pole\"] = np.where(df_flow1_clean[\"Entity\"].isin(entity_mapping.keys()), \n",
        "                                    \"X CONCESSIONS\", \n",
        "                                    df_flow1_clean[\"Entity\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Mapping of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    \"Pole\",\n",
        "    \"Entity\",\n",
        "    \"spf_mean\",\n",
        "    \"spf_score\",\n",
        "    \"spf_count\",\n",
        "    \"wp_mean\",\n",
        "    \"wp_score\",\n",
        "    \"wp_count\"\n",
        "]\n",
        "df_flow1_mapped = df_flow1_clean[column_names]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Getting the current date for snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the current UTC timestamp\n",
        "now = datetime.utcnow()\n",
        "\n",
        "# Create the folder name with today's date\n",
        "snapshot = now.strftime(\"%Y-%m-%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save CSV file in Parquet format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "file_name = (file_path.split(\"/\")[-1]\n",
        "                      .replace(\".csv\", \"\"))\n",
        "flow1_curated_path = f\"{container_curated}/{flow_folder_path}/flow-1.parquet/SNAPSHOT={snapshot}/{file_name}.snappy.parquet\"\n",
        "print(\"curated file path:\")\n",
        "print(flow1_curated_path)\n",
        "print()\n",
        "\n",
        "df_flow1_mapped.to_parquet(flow1_curated_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
